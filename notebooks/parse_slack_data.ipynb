{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataLoader\n",
    "sl = SlackDataLoader('../Anonymized_B6SlackExport_25Nov23/anonymized/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns we can get from a slack message<br>\n",
    "\n",
    "message_type, message_content, sender_id, time_sent, message_distribution, time_thread_start, reply_count, reply_user_count, time_thread_end, reply_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a single slack message, we can get <br>\n",
    "\n",
    "1. The message<br>\n",
    "2. Type (message, file, link, etc)<br>\n",
    "3. The sender_id (assigned by slack)<br>\n",
    "4. The time the message was sent<br>\n",
    "5. The team (i don't know what that is now)<br>\n",
    "6. The type of the message (broadcast message, inhouse, just messgae)<br>\n",
    "7. The thread the message generated (from here we can go):<br>\n",
    "    7.1 Text/content of the message<br>\n",
    "    7.2 The thread time of the message<br>\n",
    "    7.3 The thread count (reply count)<br>\n",
    "    7.4 The number of user that reply the message (count of users that participated in the thread)<br>\n",
    "    7.5 The time the last thread message was sent <br>\n",
    "    7.6 The users that participated in the thread (their ids are stored as well)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight Extraction\n",
    "\n",
    "Below are some useful questions to answer. Feel free to explore to answer other interesting questions that may be of help to get insight about student's behaviour, need, and future performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which user has the highest number of reply counts?\n",
    "# Assuming DataFrame is named 'df'\n",
    "df = combined_data \n",
    "user_with_highest_replies = df[df['reply_count'] == df['reply_count'].max()]['sender_name'].iloc[0]\n",
    "print(f\"The user with the highest number of reply counts is: {user_with_highest_replies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reply counts per user per channel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = combined_data \n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "sns.barplot(x='sender_name', y='reply_count', hue='channel', data=df, ci=None)\n",
    "plt.title('Reply Counts per User per Channel', size=15, fontweight='bold')\n",
    "plt.xlabel('Sender Name', size=12); plt.ylabel('Reply Count', size=12)\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "plt.legend(title='Channel', title_fontsize='12', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which user has the highest number of reply counts?# what is the time range of the day that most messages are sent?\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'])\n",
    "df['hour_of_day'] = df['msg_sent_time'].dt.hour\n",
    "\n",
    "# Plotting the distribution of messages across hours\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "sns.countplot(x='hour_of_day', data=df)\n",
    "plt.title('Distribution of Messages Across Hours of the Day', size=15, fontweight='bold')\n",
    "plt.xlabel('Hour of Day', size=12)\n",
    "plt.ylabel('Message Count', size=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of messages are replied faster than others?\n",
    "import pandas as pd\n",
    "df = combined_data \n",
    "\n",
    "def analyze_response_time(df):\n",
    "    # Convert 'tm_thread_end' and 'msg_sent_time' to datetime format if they are in string format\n",
    "    df['tm_thread_end'] = pd.to_datetime(df['tm_thread_end'], errors='coerce')\n",
    "    df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing values in 'tm_thread_end' or 'msg_sent_time'\n",
    "    df = df.dropna(subset=['tm_thread_end', 'msg_sent_time'])\n",
    "\n",
    "    # Calculate response time\n",
    "    df['response_time'] = df['tm_thread_end'] - df['msg_sent_time']\n",
    "\n",
    "    # Categorize messages based on keywords in the content\n",
    "    df['message_category'] = df['msg_content'].apply(lambda x: 'urgent' if 'urgent' in x.lower() else 'normal')\n",
    "\n",
    "    # Aggregate data based on message categories\n",
    "    result = df.groupby('message_category')['response_time'].mean()\n",
    "\n",
    "    # Print or return the result\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "analyze_response_time(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between # of messages and # of reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify messages into different categories such as questions, answers, comments, etc.\n",
    "import re\n",
    "\n",
    "def classify_message(message):\n",
    "    # Define patterns for different message categories\n",
    "    question_pattern = re.compile(r'\\b(?:how|what|when|where|why)\\b', flags=re.IGNORECASE)\n",
    "    answer_pattern = re.compile(r'\\b(?:here is|solution)\\b', flags=re.IGNORECASE)\n",
    "    comment_pattern = re.compile(r'\\b(?:nice|great|explanation)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Check for patterns and assign categories\n",
    "    if re.search(question_pattern, message):\n",
    "        return 'Question'\n",
    "    elif re.search(answer_pattern, message):\n",
    "        return 'Answer'\n",
    "    elif re.search(comment_pattern, message):\n",
    "        return 'Comment'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Example usage\n",
    "df['message_category'] = df['message_content'].apply(classify_message)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which users got the most reactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model topics mentioned in the channel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Assuming df['msg_content'] contains the messages\n",
    "documents = df['msg_content'].dropna().values.astype('U')\n",
    "\n",
    "# Create a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Apply Latent Dirichlet Allocation\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topics = lda.fit_transform(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-10-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "df['dominant_topic'] = topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the topics that got the most reactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder questions to look into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on messages, reactions, references shared, and other relevant data such as classification of questions into techical question, comment, answer, aorder stu the python, statistics, and sql skill level of a user?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
