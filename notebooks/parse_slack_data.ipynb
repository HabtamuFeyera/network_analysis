{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loader import SlackDataLoader\n",
    "from src.config import SLACK_DATA_PATH\n",
    "\n",
    "# Initialize DataLoader\n",
    "data_loader = SlackDataLoader(SLACK_DATA_PATH)\n",
    "\n",
    "# Example usage\n",
    "print(\"Channels:\", data_loader.channels)\n",
    "print(\"Users:\", data_loader.users)\n",
    "\n",
    "# Get messages from a specific channel\n",
    "channel_name = \"your_channel_name\"\n",
    "channel_messages = data_loader.get_channel_messages(channel_name)\n",
    "print(f\"Messages from {channel_name}:\", channel_messages)\n",
    "\n",
    "# User map\n",
    "print(\"User Names by ID:\", data_loader.user_names_by_id)\n",
    "print(\"User IDs by Name:\", data_loader.user_ids_by_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns we can get from a slack message<br>\n",
    "\n",
    "message_type, message_content, sender_id, time_sent, message_distribution, time_thread_start, reply_count, reply_user_count, time_thread_end, reply_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a single slack message, we can get <br>\n",
    "\n",
    "1. The message<br>\n",
    "2. Type (message, file, link, etc)<br>\n",
    "3. The sender_id (assigned by slack)<br>\n",
    "4. The time the message was sent<br>\n",
    "5. The team (i don't know what that is now)<br>\n",
    "6. The type of the message (broadcast message, inhouse, just messgae)<br>\n",
    "7. The thread the message generated (from here we can go):<br>\n",
    "    7.1 Text/content of the message<br>\n",
    "    7.2 The thread time of the message<br>\n",
    "    7.3 The thread count (reply count)<br>\n",
    "    7.4 The number of user that reply the message (count of users that participated in the thread)<br>\n",
    "    7.5 The time the last thread message was sent <br>\n",
    "    7.6 The users that participated in the thread (their ids are stored as well)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def extract_info(row):\n",
    "    return {\n",
    "        'msg_type': row.get('type', ''),\n",
    "        'msg_content': row.get('text', ''),\n",
    "        'sender_name': row.get('user_profile', {}).get('real_name', 'Not provided'),\n",
    "        'msg_sent_time': row.get('ts', ''),\n",
    "        'msg_dist_type': row.get('blocks', [{}])[0].get('elements', [{}])[0].get('elements', [{}])[0].get('type', 'reshared'),\n",
    "        'time_thread_start': row.get('thread_ts', 0),\n",
    "        'reply_count': row.get('reply_count', 0),\n",
    "        'reply_users_count': row.get('reply_users_count', 0),\n",
    "        'reply_users': ','.join(row.get('reply_users', [])),\n",
    "        'tm_thread_end': row.get('latest_reply', 0)\n",
    "    }\n",
    "\n",
    "def slack_parser(path_channel):\n",
    "    dflist = []\n",
    "\n",
    "    # Loop through each JSON file in the specified path\n",
    "    for json_file in glob.glob(f\"{path_channel}/*.json\"):\n",
    "        with open(json_file, 'r', encoding=\"utf8\") as slack_data:\n",
    "            try:\n",
    "                data = json.load(slack_data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in {json_file}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Extract required information from each message\n",
    "        messages = [extract_info(row) for row in data if 'bot_id' not in row]\n",
    "\n",
    "        # Create a DataFrame from the extracted messages\n",
    "        df = pd.DataFrame(messages)\n",
    "\n",
    "        # Filter out rows where sender name is 'Not provided'\n",
    "        df = df[df['sender_name'] != 'Not provided']\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dflist.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    dfall = pd.concat(dflist, ignore_index=True)\n",
    "\n",
    "    # Add a 'channel' column with the channel name\n",
    "    dfall['channel'] = path_channel.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # Reset the index\n",
    "    dfall = dfall.reset_index(drop=True)\n",
    "\n",
    "    return dfall\n",
    "\n",
    "# Specify the path to the directory containing JSON files\n",
    "SLACK_DATA_PATH = \"/home/habte/t\"\n",
    "\n",
    "# Call the slack_parser function\n",
    "df_combined = slack_parser(SLACK_DATA_PATH)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_timestamp(column, data):\n",
    "    \"\"\"Convert from Unix time to readable timestamp.\"\"\"\n",
    "    if column in data.columns.values:\n",
    "        timestamp_ = [datetime.datetime.fromtimestamp(float(time_unix)).strftime('%Y-%m-%d %H:%M:%S') if time_unix != 0 else 0 for time_unix in data[column]]\n",
    "        return timestamp_\n",
    "    else:\n",
    "        print(f\"{column} not in data\")\n",
    "\n",
    "def get_tagged_users(df):\n",
    "    \"\"\"Get all @ mentions in the messages.\"\"\"\n",
    "    return df['msg_content'].map(lambda x: re.findall(r'@U\\w+', x))\n",
    "\n",
    "def map_userid_to_realname(user_profile: dict, comm_dict: dict, plot=False):\n",
    "    \"\"\"\n",
    "    Map Slack ID to real names.\n",
    "    \n",
    "    Args:\n",
    "    user_profile: A dictionary that contains users' info such as real names.\n",
    "    comm_dict: A dictionary that contains Slack ID and total messages sent by that Slack ID.\n",
    "    plot: If True, plot the results.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with mapped user IDs to real names and message counts.\n",
    "    \"\"\"\n",
    "    user_dict = {i: real_name for i, real_name in zip(user_profile['id'], [dict(user_profile['profile'])[i]['real_name'] for i in range(len(user_profile['profile']))])}\n",
    "    \n",
    "    ac_comm_dict = {user_dict[i]: comm_dict[i] for i in comm_dict if i in user_dict}\n",
    "    ac_comm_dict = pd.DataFrame(data=zip(ac_comm_dict.keys(), ac_comm_dict.values()),\n",
    "                                columns=['LearnerName', '# of Msg sent in Threads']).sort_values(by='# of Msg sent in Threads', ascending=False)\n",
    "    \n",
    "    if plot:\n",
    "        ac_comm_dict.plot.bar(figsize=(15, 7.5), x='LearnerName', y='# of Msg sent in Threads')\n",
    "        plt.title('Student based on Message sent in thread', size=20)\n",
    "        \n",
    "    return ac_comm_dict\n",
    "\n",
    "def parse_slack_reaction(path, channel):\n",
    "    \"\"\"Parse Slack reactions from JSON files.\"\"\"\n",
    "    df_reaction = pd.DataFrame(columns=['reaction_name', 'reaction_count', 'reaction_users_count', 'message', 'user_id', 'channel'])\n",
    "\n",
    "    for json_file in glob.glob(f\"{path}*.json\"):\n",
    "        with open(json_file, 'r') as slack_data:\n",
    "            data = json.load(slack_data)\n",
    "\n",
    "            for message in data:\n",
    "                if 'reactions' in message:\n",
    "                    for reaction in message['reactions']:\n",
    "                        df_reaction = df_reaction.append({\n",
    "                            'reaction_name': reaction['name'],\n",
    "                            'reaction_count': reaction['count'],\n",
    "                            'reaction_users_count': len(reaction['users']),\n",
    "                            'message': message['text'],\n",
    "                            'user_id': message['user'],\n",
    "                            'channel': channel\n",
    "                        }, ignore_index=True) # type: ignore\n",
    "\n",
    "    return df_reaction\n",
    "\n",
    "def get_community_participation(path):\n",
    "    \"\"\"Get community participation from JSON files.\"\"\"\n",
    "    comm_dict = {}\n",
    "\n",
    "    for json_file in glob.glob(f\"{path}*.json\"):\n",
    "        with open(json_file, 'r') as slack_data:\n",
    "            data = json.load(slack_data)\n",
    "\n",
    "            for message in data:\n",
    "                if 'replies' in message:\n",
    "                    for reply in message['replies']:\n",
    "                        comm_dict[reply['user']] = comm_dict.get(reply['user'], 0) + 1\n",
    "\n",
    "    return comm_dict\n",
    "\n",
    "# Usage:\n",
    "directory_path = \"/home/habte/t\"\n",
    "channel_name = \"Random\"\n",
    "\n",
    "df_reaction = parse_slack_reaction(directory_path, channel_name)\n",
    "comm_dict = get_community_participation(directory_path)\n",
    "\n",
    "# Specify the path to the directory containing JSON files\n",
    "path_to_json = \"/home/habte/t\"\n",
    "\n",
    "channel_name = \"t\"\n",
    "\n",
    "# Parse reactions from Slack data\n",
    "df_reaction = parse_slack_reaction(path_to_json, channel_name)\n",
    "\n",
    "comm_dict = get_community_participation(path_to_json)\n",
    "\n",
    "# Display the dataframes\n",
    "print(\"DataFrame df_reaction:\")\n",
    "print(df_reaction.head())\n",
    "\n",
    "print(\"\\nCommunity Participation Dictionary:\")\n",
    "print(comm_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_data \n",
    "\n",
    "def get_top_users_plot(data, channel='t', top_n=20):\n",
    "    \"\"\"Plot the top message senders in a channel.\"\"\"\n",
    "    top_users = data['sender_name'].value_counts()[:top_n]\n",
    "\n",
    "    top_users.plot.bar(figsize=(15, 7.5))\n",
    "    plt.title(f'Top {top_n} Message Senders in #{channel} channel', size=15, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18)\n",
    "    plt.ylabel(\"Frequency\", size=14)\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "get_top_users_plot(df, channel='t', top_n=20)\n",
    "\n",
    "def draw_avg_reply_count(data, channel='Random'):\n",
    "    \"\"\"who commands many reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_count'].mean().sort_values(ascending=False)[:20]\\\n",
    "        .plot(kind='bar', figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_avg_reply_users_count(data, channel='Random'):\n",
    "    \"\"\"who commands many user reply?\"\"\"\n",
    "\n",
    "    data.groupby('sender_name')['reply_users_count'].mean().sort_values(ascending=False)[:20].plot(kind='bar',\n",
    "     figsize=(15,7.5));\n",
    "    plt.title(f'Average Number of reply user count per Sender in #{channel}', size=20, fontweight='bold')\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()\n",
    "\n",
    "def draw_wordcloud(msg_content, week):    \n",
    "    # word cloud visualization\n",
    "    allWords = ' '.join([twts for twts in msg_content])\n",
    "    wordCloud = WordCloud(background_color='#975429', width=500, height=300, random_state=21, max_words=500, mode='RGBA',\n",
    "                            max_font_size=140, stopwords=stopwords.words('english')).generate(allWords)\n",
    "    plt.figure(figsize=(15, 7.5))\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.title(f'WordCloud for {week}', size=30)\n",
    "    plt.show()\n",
    "\n",
    "def draw_user_reaction(data, channel='General'):\n",
    "    data.groupby('sender_name')[['reply_count', 'reply_users_count']].sum()\\\n",
    "        .sort_values(by='reply_count',ascending=False)[:10].plot(kind='bar', figsize=(15, 7.5))\n",
    "    plt.title(f'User with the most reaction in #{channel}', size=25);\n",
    "    plt.xlabel(\"Sender Name\", size=18); plt.ylabel(\"Frequency\", size=18);\n",
    "    plt.xticks(size=14); plt.yticks(size=14);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight Extraction\n",
    "\n",
    "Below are some useful questions to answer. Feel free to explore to answer other interesting questions that may be of help to get insight about student's behaviour, need, and future performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which user has the highest number of reply counts?\n",
    "# Assuming DataFrame is named 'df'\n",
    "df = combined_data \n",
    "user_with_highest_replies = df[df['reply_count'] == df['reply_count'].max()]['sender_name'].iloc[0]\n",
    "print(f\"The user with the highest number of reply counts is: {user_with_highest_replies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reply counts per user per channel\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = combined_data \n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "sns.barplot(x='sender_name', y='reply_count', hue='channel', data=df, ci=None)\n",
    "plt.title('Reply Counts per User per Channel', size=15, fontweight='bold')\n",
    "plt.xlabel('Sender Name', size=12); plt.ylabel('Reply Count', size=12)\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "plt.legend(title='Channel', title_fontsize='12', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which user has the highest number of reply counts?# what is the time range of the day that most messages are sent?\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'])\n",
    "df['hour_of_day'] = df['msg_sent_time'].dt.hour\n",
    "\n",
    "# Plotting the distribution of messages across hours\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "sns.countplot(x='hour_of_day', data=df)\n",
    "plt.title('Distribution of Messages Across Hours of the Day', size=15, fontweight='bold')\n",
    "plt.xlabel('Hour of Day', size=12)\n",
    "plt.ylabel('Message Count', size=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of messages are replied faster than others?\n",
    "import pandas as pd\n",
    "df = combined_data \n",
    "\n",
    "def analyze_response_time(df):\n",
    "    # Convert 'tm_thread_end' and 'msg_sent_time' to datetime format if they are in string format\n",
    "    df['tm_thread_end'] = pd.to_datetime(df['tm_thread_end'], errors='coerce')\n",
    "    df['msg_sent_time'] = pd.to_datetime(df['msg_sent_time'], errors='coerce')\n",
    "\n",
    "    # Drop rows with missing values in 'tm_thread_end' or 'msg_sent_time'\n",
    "    df = df.dropna(subset=['tm_thread_end', 'msg_sent_time'])\n",
    "\n",
    "    # Calculate response time\n",
    "    df['response_time'] = df['tm_thread_end'] - df['msg_sent_time']\n",
    "\n",
    "    # Categorize messages based on keywords in the content\n",
    "    df['message_category'] = df['msg_content'].apply(lambda x: 'urgent' if 'urgent' in x.lower() else 'normal')\n",
    "\n",
    "    # Aggregate data based on message categories\n",
    "    result = df.groupby('message_category')['response_time'].mean()\n",
    "\n",
    "    # Print or return the result\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "analyze_response_time(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between # of messages and # of reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify messages into different categories such as questions, answers, comments, etc.\n",
    "import re\n",
    "\n",
    "def classify_message(message):\n",
    "    # Define patterns for different message categories\n",
    "    question_pattern = re.compile(r'\\b(?:how|what|when|where|why)\\b', flags=re.IGNORECASE)\n",
    "    answer_pattern = re.compile(r'\\b(?:here is|solution)\\b', flags=re.IGNORECASE)\n",
    "    comment_pattern = re.compile(r'\\b(?:nice|great|explanation)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Check for patterns and assign categories\n",
    "    if re.search(question_pattern, message):\n",
    "        return 'Question'\n",
    "    elif re.search(answer_pattern, message):\n",
    "        return 'Answer'\n",
    "    elif re.search(comment_pattern, message):\n",
    "        return 'Comment'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Example usage\n",
    "df['message_category'] = df['message_content'].apply(classify_message)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which users got the most reactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model topics mentioned in the channel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Assuming df['msg_content'] contains the messages\n",
    "documents = df['msg_content'].dropna().values.astype('U')\n",
    "\n",
    "# Create a document-term matrix using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Apply Latent Dirichlet Allocation\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topics = lda.fit_transform(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-10-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "df['dominant_topic'] = topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the topics that got the most reactions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder questions to look into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on messages, reactions, references shared, and other relevant data such as classification of questions into techical question, comment, answer, aorder stu the python, statistics, and sql skill level of a user?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
