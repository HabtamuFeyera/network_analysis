{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "# import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all json files for all-community-building channel to dataframe using slack_parser method from loader script\n",
    "df_community = sl.slack_parser('../Anonymized_B6SlackExport_25Nov23/anonymized/all-community-building/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'message_sent_time' to datetime format\n",
    "# df['message_sent_time'] = pd.to_datetime(df['message_sent_time'])\n",
    "df_community['msg_sent_time'] = pd.to_datetime(df_community['msg_sent_time'], unit='s')\n",
    "\n",
    "# Sort the DataFrame by 'message_sent_time'\n",
    "df_community = df_community.sort_values(by='msg_sent_time')\n",
    "\n",
    "# Calculate time differences between consecutive messages\n",
    "df_community['time_difference'] = df_community['msg_sent_time'].diff()\n",
    "\n",
    "# Plot the distribution of time differences\n",
    "plt.hist(df_community['time_difference'].dt.total_seconds(), bins=50, edgecolor='black')\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Time Differences Between Messages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a histogram of the time difference between Consecutive replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the maximum reply_count\n",
    "max_reply_count= df_community['reply_count'].max()\n",
    "max_reply_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_content = sl.get_channel_messages(channel_name='all-community-building')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_msg(msg_content):\n",
    "    \n",
    "    for msg in msg_content:\n",
    "        if 'reply_count' in msg.keys():\n",
    "            if msg['reply_count']==75:\n",
    "                return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reply_count_message = filter_msg(msg_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reply_count_message = filter_msg(msg_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies['ts'] = pd.to_datetime(df_replies['ts'], unit='s')\n",
    "\n",
    "# Sort the DataFrame by 'message_sent_time'\n",
    "df_replies = df_replies.sort_values(by='ts')\n",
    "\n",
    "# Calculate time differences between consecutive messages\n",
    "df_replies['time_difference'] = df_replies['ts'].diff()\n",
    "\n",
    "# Plot the distribution of time differences\n",
    "plt.hist(df_replies['time_difference'].dt.total_seconds(), bins=50, edgecolor='black')\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Time Differences Between Messages')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim\n",
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.utils import SaveLoad\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "import pyLDAvis\n",
    "from collections import Counter\n",
    "from gensim.matutils import corpus2csc, sparse2full, corpus2dense\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_community.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocessing(df_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# clean text \n",
    "df_community = preprocess.cleantext('msg_content', 'clean_msg_content')\n",
    "\n",
    "# Stem words\n",
    "df_community = preprocess.stem('clean_msg_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used stemmer instead of lemmatizer\n",
    "cleanKagslacklist = preprocess.filterSlackList(df_community['clean_msg_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanKagslacklist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(mySlackList):\n",
    "    \"\"\"Create dictionary from list of tokenized documents\"\"\"\n",
    "    return corpora.Dictionary(mySlackList)\n",
    "\n",
    "def makeCorpus(mySlackList,myDict):\n",
    "    \"\"\"Create corpus from list of tokenized documents\"\"\"\n",
    "    return [myDict.doc2bow(slackmessage) for slackmessage in mySlackList]\n",
    "\n",
    "def createLDA(myCorpus, myDictionary,myTopics=50,myPasses=10,myIterations=50,myAlpha=0.001):\n",
    "    \"\"\"LDA model call function\"\"\"\n",
    "    return models.LdaMulticore(myCorpus, id2word=myDictionary, num_topics=myTopics, passes=myPasses,\n",
    "    iterations=myIterations,alpha=myAlpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create model objects\"\"\"\n",
    "kagDict   = makeDict(cleanKagslacklist)\n",
    "kagCorpus = makeCorpus(cleanKagslacklist, kagDict)\n",
    "kagLda = createLDA(kagCorpus, kagDict)\n",
    "\n",
    "\"\"\"Save model objects\"\"\"\n",
    "SaveLoad.save(kagLda,'kaggleLDAmodel')\n",
    "corpora.MmCorpus.serialize('kaggleCorpus.mm', kagCorpus)\n",
    "kagDict.save('kaggleDictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kagLda = SaveLoad.load('kaggleLDAmodel')\n",
    "kagDict = corpora.Dictionary.load('kaggleDictionary.dict')\n",
    "kagCorpus = corpora.MmCorpus('kaggleCorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kagLda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "for topic_id, topic in kagLda.show_topics(num_topics=num_topics, formatted=False):\n",
    "    print(f\"Topic {topic_id + 1}: {', '.join([word[0] for word in topic])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateLdaIdx(myLdaModel, myLdaViz):\n",
    "    \"\"\"Translate lda model topics to match the topics in pyLDAvis visualization\"\"\"\n",
    "    ldaVizIdx = myLdaViz[0].index\n",
    "    return list(ldaVizIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newIdx = translateLdaIdx(kagLda,ldaViz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDenseMat(myLdaModel,myCorpus,newIdx):\n",
    "    \"\"\"Transform corpus to dataframe with topics matching lda visualization\"\"\"\n",
    "    numTopics = myLdaModel.num_topics\n",
    "    myDense = corpus2dense(myLdaModel[myCorpus],numTopics)\n",
    "    myDf = pd.DataFrame(myDense)\n",
    "    mySortedDf = myDf.transpose()\n",
    "    mySortedDf = myDf.transpose()[newIdx]\n",
    "    mySortedDf.columns = ['topic' + str(i + 1) for i in range(numTopics)]\n",
    "    return mySortedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kagDf = createDenseMat(kagLda,kagCorpus,newIdx)\n",
    "def sortByTopicToIdx(cleanKagslacklist,mySortedDf,myTopic,myTopicThresh=0.1):\n",
    "    \"\"\"Returns an index of tweets surpassing a topic value threshold\"\"\"\n",
    "    srtIdx = list(mySortedDf[mySortedDf[myTopic]>myTopicThresh].index)\n",
    "    return srtIdx\n",
    "\n",
    "def sortSlackByIdx(cleanKagslacklist,srtIdx):\n",
    "    \"\"\"Returns sorted tweets as a list based on a defined sort index\"\"\"\n",
    "    myCleanArray = np.array(cleanKagslacklist)\n",
    "    srtTweets = list(myCleanArray[srtIdx])\n",
    "    return srtTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sortedIdx = sortByTopicToIdx(cleanKagslacklist,kagDf,'topic2',myTopicThresh=0.1)# Word cloud for topic \n",
    "def makeWordCloud(cleanKagslacklist,mySortedDf,myTopic,myTopicThresh=0.1):\n",
    "    \"\"\"Create word cloud of tweets passing a given threshold for a given topic\"\"\"\n",
    "    sortedIdx = sortByTopicToIdx(cleanKagslacklist,mySortedDf,myTopic,myTopicThresh=0.1)\n",
    "    mySortedSlack = sortSlackByIdx(cleanKagslacklist,sortedIdx)\n",
    "    filteredWords = ' '.join([' '.join(filter(None, string)) for string in mySortedSlack])\n",
    "    myTopicCloud = WordCloud(width=800, height=800, max_font_size=100, scale=8).generate(filteredWords)\n",
    "\n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 10), dpi=1600)\n",
    "    plt.imshow(myTopicCloud)\n",
    "    plt.axis(\"off\")  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordCloud(cleanKagslacklist, mySortedDf, myTopic, myTopicThresh=0.1):\n",
    "    \"\"\"Create word cloud of tweets passing a given threshold for a given topic\"\"\"\n",
    "    sortedIdx = sortByTopicToIdx(cleanKagslacklist, mySortedDf, myTopic, myTopicThresh=0.1)\n",
    "    mySortedSlack = sortSlackByIdx(cleanKagslacklist, sortedIdx)\n",
    "    filteredWords = ' '.join([' '.join(filter(None, string)) for string in mySortedSlack])\n",
    "    myTopicCloud = WordCloud(width=800, height=800, max_font_size=100, scale=8).generate(filteredWords)\n",
    "\n",
    "    # Display the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(myTopicCloud)\n",
    "    plt.axis(\"off\")  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeWordCloud(cleanKagslacklist,kagDf,'topic2',myTopicThresh=0.2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
